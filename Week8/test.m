%%%%%%%%%%%%%%%%%%%%%%%%%%%                        %%  Exercice for K-means  %%                        %%%%%%%%%%%%%%%%%%%%%%%%%%%% Initialize training dataXa = [ 50.*randn(45,1) + 400, 60.*randn(45,1) + 800];Xb = [ 60.*randn(25,1) + 100, 40.*randn(25,1) + 300];Xc = [ 75.*randn(50,1) + 900, 85.*randn(50,1) + 800];Xd = [ 100.*randn(30,1) + 700, 80.*randn(30,1) + 200];X = [Xa; Xb; Xc; Xd];% Nb of data inputI = 150;% Cluster assigned to each data inputY = zeros(I, 1);% K-means algorithm% Initialize cluster centroidsK = 4; % nb of clustersCc = 1000*rand(K, 2); % Cluster centroidsCc_next = Cc;% Delta to checks the convergencedelta = 1000;% Repeat until convergencewhile delta > 0.0001  delta = 0;    % For each output  for i = 1:I    %Update y(i)    min = 100000;    % For each cluster    for k = 1:K      % Compute distance between input and cluster centroid      dist = sqrt((X(i, 1)-Cc(k,1))^2 + (X(i, 2)-Cc(k,2))^2);      if dist < min        min = dist;        Y(i) = k;      end    end  end    % For each cluster  for k = 1:K    % Update each centroid        sum = zeros(1, 2); % Coordinates sum of elements for this cluster    counter = 0; % Count nb of elements for this cluster    % For each input    for i = 1:I      % If element classified for the cluster      if Y(i) == k        sum += X(i, :);        counter++;      end    end        % Update centroid    if counter > 0      Cc_next(k, :) = sum / counter;    else      Cc_next(k, :) = 1000*rand(1, 2);    end        % Update delta    if(sqrt((Cc(k, 1)-Cc_next(k,1))^2 + (Cc(k, 2)-Cc_next(k,2))^2) > delta)      delta = sqrt((Cc(k, 1)-Cc_next(k,1))^2 + (Cc(k, 2)-Cc_next(k,2))^2);    end  end  Cc = Cc_next;end% Plot the resultfigure;plot(X(Y==1,1),X(Y==1,2),'m.','MarkerSize', 5)hold onplot(X(Y==2,1),X(Y==2,2),'b.','MarkerSize', 5)hold onplot(X(Y==3,1),X(Y==3,2),'g.','MarkerSize', 5)hold onplot(X(Y==4,1),X(Y==4,2),'r.','MarkerSize', 5)plot(Cc(:,1),Cc(:,2),'kx', 'MarkerSize',15,'LineWidth',3)legend('Cluster 1','Cluster 2', 'Cluster 3', 'Cluster 4', 'Centroids', 'Location', 'northwest')title 'Cluster Assignments and Centroids';hold off% Test Model with new DataXtesta = [ 50.*randn(10,1) + 400, 60.*randn(10,1) + 700];Xtestb = [ 60.*randn(10,1) + 200, 40.*randn(10,1) + 300];Xtestc = [ 75.*randn(10,1) + 900, 85.*randn(10,1) + 800];Xtestd = [ 50.*randn(10,1) + 800, 50.*randn(10,1) + 200];Xtest = [Xtesta; Xtestb; Xtestc; Xtestd];% Nb of data inputI = 40;% Cluster assigned to each data inputYtest = zeros(I, 1);% For each outputfor i = 1:I  %Update y(i)  min = 100000;  % For each cluster  for k = 1:K    % Compute distance between input and cluster centroid    dist = sqrt((Xtest(i, 1)-Cc(k,1))^2 + (Xtest(i, 2)-Cc(k,2))^2);    if (dist < min)      min = dist;      Ytest(i) = k;    end  endend% Plot the clustering result for the test datafigure;plot(Xtest(Ytest==1,1),Xtest(Ytest==1,2),'m.','MarkerSize', 5)hold onplot(Xtest(Ytest==2,1),Xtest(Ytest==2,2),'b.','MarkerSize', 5)hold onplot(Xtest(Ytest==3,1),Xtest(Ytest==3,2),'g.','MarkerSize', 5)hold onplot(Xtest(Ytest==4,1),Xtest(Ytest==4,2),'r.','MarkerSize', 5)plot(Cc(:,1),Cc(:,2),'kx', 'MarkerSize',15,'LineWidth',3)legend('Cluster 1','Cluster 2', 'Cluster 3', 'Cluster 4', 'Centroids', 'Location', 'northwest')title 'Cluster Assignments and Centroids for test data';hold off%%%%%%%%%%%%%%%%%%%%%%%%%%%                        %%  Exercice for PCA      %%                        %%%%%%%%%%%%%%%%%%%%%%%%%%%% Generate yourself some data following a 2D linear model with some random noise.I = 50;% input data: follows a 2D linear model with some random noiseX1 = 20 + 20*rand(I, 1); % abscissa (10 different abcissa from 20 to 40noise = -2.5 + 5*rand(I, 1); % noise from -2.5 to 2.5X2 = 2*X1 + 30 + noise; % follows linear function X2 = 2*X + 5 whith some noise addedX = [X1 X2];% Plot the clustering result for the test datafigure;plot(X(:,1),X(:,2),'b.','MarkerSize', 5)hold on% PCA algorithm% Pre-processing data% Set Meanmn = mean(X);% Substract the mean to the dataX -= mn;% Compute covariance matrixsigma = 1/I * X' * X;% Compute the K-most significant eigenvectors of covariance[U, S, V] = svd(sigma); % U: right eigenvectors, V: eigenvalues% Find the most significant eigenvector -> This is the principal axesU = U(:, 1);% Project the data set onto UY = X * U;plot(Y(:,1), zeros(I, 1), 'c.','MarkerSize', 5)hold on% Test Model with new Data% input data: follows a 2D linear model with some random noiseI = 20;Xtest1 = 10*rand(I, 1); % abscissa (10 different abcissa from 0 to 10noise = -5 + 10*rand(I, 1); % noise from -5 to 5Xtest2 = 2*Xtest1 + 30 + noise; % follows linear function X2 = 2*X + 5 whith some noise addedXtest = [Xtest1 Xtest2];plot(Xtest(:,1),Xtest(:,2),'r.','MarkerSize', 5)hold on% Normalize test dataX -= mn;% Project the test data set onto UY = Xtest * U;plot(Y(:,1), zeros(I, 1), 'm.','MarkerSize', 5)hold on% Plot the principal axet = -80:40;plot(t'*U'(:, 1), t'*U'(:, 2), 'g');legend('Data', 'Output', 'Test data', 'Test Output', 'Principal Axe', 'Location', 'northwest')title 'Data and data preprocessed for PCA algorithm';hold off